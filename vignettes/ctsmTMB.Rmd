---
title: "Getting Started"
author: ""
date: ""
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Getting Started}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

Let's get starting using `ctsmTMB`.

```{r, include=TRUE}
library(ctsmTMB)
```

We shall assume that you have some time series data with observations and model inputs, and have a continous-time stochastic differential equation which you believe can be used to model some latent variable state which in turn can be linked to the gathered observations.

# Model

In this example we consider a simple mean-reverting model based on the Ornstein-Uhlenbeck process
$$ 
\mathrm{d}x_{t} = \theta (\mu - x_{t}) \, \mathrm{d}t \, + \sigma_{x} \, \mathrm{d}b_{t} 
$$
where $\theta$, $\mu$ and $\sigma_x$ are (fixed effects) parameters in the model to be estimated. 
  
We assume that we have direct observations of the state $x_{t}$ i.e. the observation equation is
$$
y_{t_{k}} = x_{t_{k}} + \varepsilon_{t_{k}} \qquad \varepsilon_{t_{k}} \sim N(0,\sigma_{y}^{2} \cdot u_{t_{k}})
$$
and the residuals are normally distributed with variance $\sigma_{y}^{2} \cdot u_{t}$, where $\sigma_{y}$ is a fixed effect (parameter) which should also be estimated, and $u_{t}$ is an input. Typically this input is not added, but it is done so here for sake of this example, to demonstrate how inputs are specified. It's effect here could e.g. be as the vector 
$$
u_{t_{i}} = \left\{ 1, 4, 1, 4, 1, 4 \cdots \right\}
$$
so that some observations have twice the standard deviation of others.

# Initialising
  
We initialise a `ctsmTMB` model object using

```{r}
model = ctsmTMB$new()
```

<!-- We set the model name, which is used to generate the underlying C++ files -->

<!-- ```{r} -->
<!-- model$setModelname("getting_started_model") -->
<!-- ``` -->

<!-- By default the files are saved in a directory called *ctsmTMB_cppfiles* in the current working directory. The files can be stored in another location by specifying another directory as below: -->

<!-- ```{r, eval=FALSE} -->
<!-- model$setCppfilesDirectory("some_path_to_a_directory") -->
<!-- ``` -->

```{r, include=FALSE, eval=FALSE}
temporary_directory <- file.path(tempdir(),"ctsmTMB_cppfiles")
model$setCppfilesDirectory(temporary_directory)
```

Now that the object is created let us first inspect it's printout:

```{r}
print(model)
```

We see that the model is called `ctsmTMB_model` (default), and there are no states, diffusions, observations, inputs or parameters currently registered. 

## **Note on the model name**

The model name is only relevant for the few methods that generate C++ files (`ekf_cpp` and `ukf_cpp`) where it is used to name the `C++` file(s) that will be created locally, and to later recognize the compiled model object. The model name and directory where C++ files are stored, can be set using:

```{r, eval=FALSE}
model$setModelname("getting_started")
model$setCppfilesDirectory("some_path_to_a_directory")
```

# Add system equations

---

We can begin by adding the desired stochastic differential equation to the object, which is straighforward:
```{r}
model$addSystem(dx ~ theta * (mu - x) * dt + sigma_x * dw)
```
We note that drift terms are specified in that they have a `* dt` and similary for diffusions with `* dw` or `* dw#` where `#` can be any sequence of numbers. A single equation can contain any number of
diffusion terms i.e.

```{r, eval=FALSE}
model$addSystem(dx ~ theta * (mu - x) * dt + sigma_x * dw + sigma_x2 * dw2)
```

# Add observation equations

---

Next we add the observation equations:
```{r}
model$addObs(y ~ x)
```
Here we are saying that the latent variable `x` is observed (directly) by the observations `y`. The observations for `y` are now identified in the (to-be provided) estimation data via the name "y".

# Add observation variances

---

We must also specify the variance of the normally distributed residuals for each observation equation. This is done as follows:
```{r}
model$setVariance(y ~ sigma_y^2 * u)
```
The variable name `y` on the left-hand side of the formula must match a name previously defined via `addObs`. 

Let's inspect the model object again
```{r}
print(model)
```
So now we have specified one state $x$ and an observation equation $y$. We also note that there are no inputs and no parameter specified yet.

# Add inputs

---

We tell the model which variable names are inputs via
```{r}
model$addInput(u)
```
The input values should be provided in the estimation data, using the same name, exactly similar to how observations are provided.


# Add parameters

---

We must also specify the (fixed effects) parameters, and in addition their values (initial, lower/upper bound) or the estimation procedure (likelihood optimization).
```{r}
model$setParameter(
  theta   = c(initial = 5,    lower = 0,    upper = 20),
  mu      = c(initial = 0,    lower = -10,  upper = 10),
  sigma_x = c(initial = 1e-1, lower = 1e-5, upper = 5),
  sigma_y = c(initial = 1e-1, lower = 1e-5, upper = 5)
)
```

We can fix a parameter value (so the parameter becomes a constant) by supplying just a single value. 

It is for instance typically useful to fix some noise parameters, because they can be difficult to identify simultaneously. Here we fix the observation noise $\sigma_{y}$:
```{r}
model$setParameter(
  sigma_y  = 1e-1
)
```

Let's inspect the model object again, and see that inputs and parameters (both non-fixed and fixed) have been registered.
```{r}
print(model)
```

# Set initial state and covariance

---

Finally, we must set the state's expected value and (co)variance at the initial time-point, where the filtering algorithms begin, with the assumption that the state is normally distributed. The choice of values should reflect ones belief in the initial state, but it is probably just a ballpark figure.
```{r}
x0 <- 3
p0 <- 1e-1 * diag(1)
initial.state <- list(x0, p0)
model$setInitialState(initial.state)
```
Note that we use `diag` to construct a 1x1 matrix for the covariance, as required by the method.


# Fit model parameters to data

---

We are now ready to perform state filtration and parameter estimation. We first construct some fake data by simulating paths of the Ornstein-Uhlenbeck process using the [Euler-Maruyama scheme](https://en.wikipedia.org/wiki/Eulerâ€“Maruyama_method).

```{r, fig.height=7,fig.width=7}
library(ggplot2)
set.seed(10)
simulation.pars <- c(theta=10, mu=1, sigma_x=1, sigma_y=1e-1)
dt.sim <- 1e-3
t.sim <- seq(0,1,by=dt.sim)
df.sim <- data.frame(t=t.sim, u=1, y=0)

# Perform simulation
sim <- model$simulate(data=df.sim, pars=simulation.pars, n.sims=1, silent=T)
x <- sim$states$x$i0$x1

# Extract observations from simulation and add noise
dt.obs = 1e-2
t.obs = seq(0,1,by=dt.obs)
y = x[t.sim %in% t.obs] + simulation.pars["sigma_y"] * rnorm(length(t.obs))

data = data.frame(
  t = t.obs,
  u = 1,
  y = y
)

ggplot() +
  geom_line(aes(x=t.sim,y=x,color="Simulation")) +
  geom_point(aes(x=t.obs,y=y,fill="Observations")) +
  ctsmTMB:::getggplot2theme() + labs(color="",fill="")
```


```{r, fig.height=7,fig.width=7, eval=FALSE, include=FALSE}
# Choosing parameters
set.seed(10)
theta=10; mu=1; sigma_x=1; sigma_y=1e-1

# Creating simulation path
dt.sim = 1e-3
t.sim = seq(0,1,by=dt.sim)
dw = rnorm(length(t.sim)-1,sd=sqrt(dt.sim))
x = 3
for(i in 1:(length(t.sim)-1)) {
  x[i+1] = x[i] + theta*(mu-x[i])*dt.sim + sigma_x*dw[i]
}

# Extract observations from simulation and add noise
dt.obs = 1e-2
t.obs = seq(0,1,by=dt.obs)
y = x[t.sim %in% t.obs] + sigma_y * rnorm(length(t.obs))

# Create data.frame
data = data.frame(
  t = t.obs,
  y = y,
  u = c(rep(c(1,2),times=round(length(y)/2)),1)
)
```

The data must contain a time column named `t` and columns for each of the specified inputs and observations.

We pass the `data` to the `estimate` method. The method will build and compile a generated `C++` function for the negative log-likelihood, check if the supplied data contains all necessary variables, construct the objective function (the computational tree must be build for automatic differentiation) and then start the optimization. 

The output generated during the optimization is the objective (negativ log-likelihood) value and parameter values at the current step. The optimizer used in the package is `stats::nlminb`. This optimizer is great because of its robustness and ability to use the objective function hessian unlike e.g. 'stats::optim'

```{r}
fit = model$estimate(data)
```

# Parameter estimates

---

Let's inspect results from the estimation.

We can print the `fit` object to see a standard coefficient matrix for the parameter estimates.
```{r}
print(fit)
```
We can see the parameter estimate and the associated standard error together with the t-test statistic and P-value associated with the standard null-hypothesis 
$$
H_{0}: p = 0 \\
H_{1}: p \neq 0
$$
Note that the true parameter values were set during the simulation step as $\theta = 10$, $\mu=1$ and $\sigma_{X} = 1$.

The parameter values, standard deviations and covariance matrix can be extracted via:

The estimated (fixed) parameters:
```{r}
fit$par.fixed
```

The standard deviations of the (fixed) parameters:
```{r}
fit$sd.fixed
```

The covariance of the (fixed) parameters:
```{r}
fit$cov.fixed
```

# State estimates

---

We can also plot prior and posterior state estimates The prior state estimate is the resulting estimate purely from integrating the mean and covariance of the SDE system forward in time, while the posterior state estimation is obtained from updating the prior estimate with the information contained in the observation (using Bayes' rule).

```{r}
library(ggplot2)
library(patchwork)
```

```{r, fig.height=7,fig.width=7}
t         = fit$states$mean$posterior$t
xprior    = fit$states$mean$prior$x
xpost     = fit$states$mean$posterior$x
xpost_sd  = fit$states$sd$posterior$x

ggplot() +
  geom_line(aes(x=t,y=xpost,color="State Estimates (Posterior)"),lwd=1) +
  geom_line(aes(x=t,y=xprior,color="State Estimates (Prior)"),lwd=1) +
  geom_ribbon(aes(x=t,ymin=xpost-2*xpost_sd,ymax=xpost+2*xpost_sd),fill="grey",alpha=0.5) +
  geom_point(aes(x=data$t,data$y,color="Observations")) +
  guides(color=guide_legend(override.aes=list(shape=c(16,NA,NA),size=c(2,NA,NA),linetype=c(NA,1,1),lwd=c(NA,1,1)))) +
  labs(x = "Time", y = "", color="") +
  ctsmTMB:::getggplot2theme()
```

# Residual analysis

---

We can display a standard residual analysis for the observations by calling `plot` on the `fit` object, which invokes the `S3` `plot.ctsmTMB.fit` method of `plot`. This includes a quantile-quantile plot, histogram, auto-correlations and cumulative periodogram.

```{r, fig.height=7, fig.width=7}
plot(fit)
```
The residuals (both standard and normalized) can be extracted as well, together with their standard deviation and covariance matrix via `fit$residuals`

# **Extra:** Adding algebraic equations

---

In order to keep the code clean, one can define algebraic expressions, that replace variables in the defined equations. As an example estimating $\theta$ as strictly positive corresponds to estimating $\log(\theta)$ so really we are looking for the parameter $\hat{\theta} = \exp(\log(\theta))$. This can be achieved by setting the following algrabic expression:
```{r}
model$setAlgebraics(theta ~ exp(logtheta))
```

A new parameter entry must be added to the model object which describes values for `logtheta` which has replaced `theta`:
```{r}
model$setParameter(logtheta = log(c(initial=5, lower=0, upper=20)))
```

